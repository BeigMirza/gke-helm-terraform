{
  "version": 4,
  "terraform_version": "1.9.2",
  "serial": 84,
  "lineage": "1ac3afaa-1b96-083f-0715-8b4a9b20b548",
  "outputs": {},
  "resources": [
    {
      "mode": "data",
      "type": "google_client_config",
      "name": "default",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "access_token": "ya29.c.c0ASRK0GZ3lbytZmsTOoRsLX0p4Z0RyvGZnSQSqBYHTZWoRcpOlPkrOCOdS81lOCCvOcgbZNmpim50Q_7dihrZnAeaPXy0H4F_XbIk86XGJEr4eKk8JbPAOT7Lgrqf-V3DYu6qpVzFyMHeFjvxFr4I0HdIJiJVb_bSAVsLGNNmLJoH_kkrrxJ7J5YEP_eO7mc4aDF99bhlgklOV8dzjq0OqLpjIbOmVMBkbSqor4Pgg-ccgy7MKL376WuvXK3LeYZs-Bw0ilq-diueTnso5VdR9cVfkq8z4KaA3ZHlCR3Xs6fRXbfhLqFgmkKux_ItNDZmDICFyR6IGYrDDlHrIovR3_X1Y4fpEIEsk0HomN2sqy_hN0si-9H_DXBjS0cG388Am-tQzoRwbxwbfpya9646bXUdOl71aOblURSmb17Jdf65WrRSI-eVbs_xbXX00S7-cbR6vRjey1lterxko_sYM0fB-xrRgi3_2utbbZ12oJS1n9ppevfB5FeWRMz_h5pdi-tM9Uh6qnVScMZt-gS42I42Mb1fQ7W1dVkVlt9kd0y9q1znWlx6Yjgw3c3j5taUQm_MraXjcnXFWn3XU6u7wUhFFYQ9jMrf6idnqfB-gV6s539zBSBchvr7-zbes4VIrd43Rckf0MifiObiopmYh4VUtdY8-7nkn3s3i4icM-jly_glySqc1liiOzbnJUeRXUZsjk-dbXnbeUlsiiZzi39RpwY7nrms-vZXlhzQo8vhca2bIWp5J-eukljoxWQ1Qykvoc8_hz83QeF7Mm7rRZ1fb5fiJJX75n5z95Qh6a53ll5jushkiecpJikz_R7xq4SsUXr5saU6QMmhOIMqOQ7d2-4IBXc3jti1Z3we0axXBac_h_zmdbBkvv-WBi3wuMzW1lbdbft-qtt1_XXlngfRy6Bz35wlIY5nc76sVeF75a30XVh-9bguz3gs7-i5w07wjkIxyoqZ81-_UkYcB9iz_bmkRcSt1gvwvfubSepOmF9QiByZBwt",
            "id": "projects/\"gke-helm-431811\"/regions/\"asia-south1\"/zones/\"asia-south1-a\"",
            "project": "gke-helm-431811",
            "region": "asia-south1",
            "zone": "asia-south1-a"
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "access_token"
              }
            ]
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "google_container_cluster",
      "name": "primary",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "schema_version": 2,
          "attributes": {
            "addons_config": [
              {
                "cloudrun_config": [],
                "config_connector_config": [],
                "dns_cache_config": [],
                "gce_persistent_disk_csi_driver_config": [
                  {
                    "enabled": true
                  }
                ],
                "gcp_filestore_csi_driver_config": [],
                "gcs_fuse_csi_driver_config": [],
                "gke_backup_agent_config": [],
                "horizontal_pod_autoscaling": [],
                "http_load_balancing": [],
                "network_policy_config": [
                  {
                    "disabled": true
                  }
                ],
                "stateful_ha_config": []
              }
            ],
            "allow_net_admin": null,
            "authenticator_groups_config": [],
            "binary_authorization": [
              {
                "enabled": false,
                "evaluation_mode": ""
              }
            ],
            "cluster_autoscaling": [
              {
                "auto_provisioning_defaults": [],
                "autoscaling_profile": "BALANCED",
                "enabled": false,
                "resource_limits": []
              }
            ],
            "cluster_ipv4_cidr": "10.56.0.0/14",
            "confidential_nodes": [],
            "cost_management_config": [],
            "database_encryption": [
              {
                "key_name": "",
                "state": "DECRYPTED"
              }
            ],
            "datapath_provider": "",
            "default_max_pods_per_node": 110,
            "default_snat_status": [
              {
                "disabled": false
              }
            ],
            "deletion_protection": false,
            "description": "",
            "dns_config": [],
            "enable_autopilot": false,
            "enable_cilium_clusterwide_network_policy": false,
            "enable_intranode_visibility": false,
            "enable_k8s_beta_apis": [],
            "enable_kubernetes_alpha": false,
            "enable_l4_ilb_subsetting": false,
            "enable_legacy_abac": false,
            "enable_shielded_nodes": true,
            "enable_tpu": false,
            "endpoint": "34.93.171.75",
            "fleet": [],
            "gateway_api_config": [],
            "id": "projects/gke-helm-431811/locations/asia-south1/clusters/my-gke-cluster",
            "identity_service_config": [],
            "initial_node_count": 1,
            "ip_allocation_policy": [
              {
                "additional_pod_ranges_config": [],
                "cluster_ipv4_cidr_block": "10.56.0.0/14",
                "cluster_secondary_range_name": "gke-my-gke-cluster-pods-b3a741d5",
                "pod_cidr_overprovision_config": [
                  {
                    "disabled": false
                  }
                ],
                "services_ipv4_cidr_block": "34.118.224.0/20",
                "services_secondary_range_name": "",
                "stack_type": "IPV4"
              }
            ],
            "label_fingerprint": "a9dc16a7",
            "location": "asia-south1",
            "logging_config": [
              {
                "enable_components": [
                  "SYSTEM_COMPONENTS",
                  "WORKLOADS"
                ]
              }
            ],
            "logging_service": "logging.googleapis.com/kubernetes",
            "maintenance_policy": [],
            "master_auth": [
              {
                "client_certificate": "",
                "client_certificate_config": [
                  {
                    "issue_client_certificate": false
                  }
                ],
                "client_key": "",
                "cluster_ca_certificate": "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"
              }
            ],
            "master_authorized_networks_config": [],
            "master_version": "1.29.6-gke.1326000",
            "mesh_certificates": [],
            "min_master_version": null,
            "monitoring_config": [
              {
                "advanced_datapath_observability_config": [
                  {
                    "enable_metrics": false,
                    "enable_relay": false,
                    "relay_mode": "DISABLED"
                  }
                ],
                "enable_components": [
                  "SYSTEM_COMPONENTS",
                  "STORAGE",
                  "HPA",
                  "POD",
                  "DAEMONSET",
                  "DEPLOYMENT",
                  "STATEFULSET",
                  "CADVISOR",
                  "KUBELET"
                ],
                "managed_prometheus": [
                  {
                    "enabled": true
                  }
                ]
              }
            ],
            "monitoring_service": "monitoring.googleapis.com/kubernetes",
            "name": "my-gke-cluster",
            "network": "projects/gke-helm-431811/global/networks/default",
            "network_policy": [
              {
                "enabled": false,
                "provider": "PROVIDER_UNSPECIFIED"
              }
            ],
            "networking_mode": "VPC_NATIVE",
            "node_config": [],
            "node_locations": [
              "asia-south1-a",
              "asia-south1-b",
              "asia-south1-c"
            ],
            "node_pool": [],
            "node_pool_auto_config": [],
            "node_pool_defaults": [
              {
                "node_config_defaults": [
                  {
                    "containerd_config": [],
                    "logging_variant": "DEFAULT"
                  }
                ]
              }
            ],
            "node_version": "1.29.6-gke.1326000",
            "notification_config": [
              {
                "pubsub": [
                  {
                    "enabled": false,
                    "filter": [],
                    "topic": ""
                  }
                ]
              }
            ],
            "operation": null,
            "private_cluster_config": [
              {
                "enable_private_endpoint": false,
                "enable_private_nodes": false,
                "master_global_access_config": [
                  {
                    "enabled": false
                  }
                ],
                "master_ipv4_cidr_block": "",
                "peering_name": "",
                "private_endpoint": "10.160.0.16",
                "private_endpoint_subnetwork": "",
                "public_endpoint": "34.93.171.75"
              }
            ],
            "private_ipv6_google_access": "",
            "project": "gke-helm-431811",
            "release_channel": [
              {
                "channel": "REGULAR"
              }
            ],
            "remove_default_node_pool": true,
            "resource_labels": null,
            "resource_usage_export_config": [],
            "security_posture_config": [
              {
                "mode": "BASIC",
                "vulnerability_mode": "VULNERABILITY_MODE_UNSPECIFIED"
              }
            ],
            "self_link": "https://container.googleapis.com/v1/projects/gke-helm-431811/locations/asia-south1/clusters/my-gke-cluster",
            "service_external_ips_config": [
              {
                "enabled": false
              }
            ],
            "services_ipv4_cidr": "34.118.224.0/20",
            "subnetwork": "projects/gke-helm-431811/regions/asia-south1/subnetworks/default",
            "timeouts": null,
            "tpu_ipv4_cidr_block": "",
            "vertical_pod_autoscaling": [],
            "workload_identity_config": []
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "master_auth"
              },
              {
                "type": "index",
                "value": {
                  "value": 0,
                  "type": "number"
                }
              },
              {
                "type": "get_attr",
                "value": "client_key"
              }
            ]
          ],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoyNDAwMDAwMDAwMDAwLCJkZWxldGUiOjI0MDAwMDAwMDAwMDAsInJlYWQiOjI0MDAwMDAwMDAwMDAsInVwZGF0ZSI6MzYwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMiJ9"
        }
      ]
    },
    {
      "mode": "managed",
      "type": "google_container_node_pool",
      "name": "primary_preemptible_nodes",
      "provider": "provider[\"registry.terraform.io/hashicorp/google\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "autoscaling": [],
            "cluster": "my-gke-cluster",
            "id": "projects/gke-helm-431811/locations/asia-south1/clusters/my-gke-cluster/nodePools/my-node-pool",
            "initial_node_count": 1,
            "instance_group_urls": [
              "https://www.googleapis.com/compute/v1/projects/gke-helm-431811/zones/asia-south1-b/instanceGroupManagers/gke-my-gke-cluster-my-node-pool-831a3b9d-grp",
              "https://www.googleapis.com/compute/v1/projects/gke-helm-431811/zones/asia-south1-a/instanceGroupManagers/gke-my-gke-cluster-my-node-pool-7b84e614-grp",
              "https://www.googleapis.com/compute/v1/projects/gke-helm-431811/zones/asia-south1-c/instanceGroupManagers/gke-my-gke-cluster-my-node-pool-78de03f6-grp"
            ],
            "location": "asia-south1",
            "managed_instance_group_urls": [
              "https://www.googleapis.com/compute/v1/projects/gke-helm-431811/zones/asia-south1-b/instanceGroups/gke-my-gke-cluster-my-node-pool-831a3b9d-grp",
              "https://www.googleapis.com/compute/v1/projects/gke-helm-431811/zones/asia-south1-a/instanceGroups/gke-my-gke-cluster-my-node-pool-7b84e614-grp",
              "https://www.googleapis.com/compute/v1/projects/gke-helm-431811/zones/asia-south1-c/instanceGroups/gke-my-gke-cluster-my-node-pool-78de03f6-grp"
            ],
            "management": [
              {
                "auto_repair": true,
                "auto_upgrade": true
              }
            ],
            "max_pods_per_node": 110,
            "name": "my-node-pool",
            "name_prefix": "",
            "network_config": [
              {
                "create_pod_range": false,
                "enable_private_nodes": false,
                "network_performance_config": [],
                "pod_cidr_overprovision_config": [],
                "pod_ipv4_cidr_block": "10.56.0.0/14",
                "pod_range": "gke-my-gke-cluster-pods-b3a741d5"
              }
            ],
            "node_config": [
              {
                "advanced_machine_features": [],
                "boot_disk_kms_key": "",
                "confidential_nodes": [],
                "containerd_config": [],
                "disk_size_gb": 100,
                "disk_type": "pd-balanced",
                "effective_taints": [],
                "enable_confidential_storage": false,
                "ephemeral_storage_local_ssd_config": [],
                "fast_socket": [],
                "gcfs_config": [],
                "guest_accelerator": [],
                "gvnic": [],
                "host_maintenance_policy": [],
                "image_type": "COS_CONTAINERD",
                "kubelet_config": [],
                "labels": {},
                "linux_node_config": [],
                "local_nvme_ssd_block_config": [],
                "local_ssd_count": 0,
                "logging_variant": "DEFAULT",
                "machine_type": "e2-small",
                "metadata": {
                  "disable-legacy-endpoints": "true"
                },
                "min_cpu_platform": "",
                "node_group": "",
                "oauth_scopes": [
                  "https://www.googleapis.com/auth/cloud-platform"
                ],
                "preemptible": true,
                "reservation_affinity": [],
                "resource_labels": null,
                "resource_manager_tags": null,
                "secondary_boot_disks": [],
                "service_account": "helm-374@gke-helm-431811.iam.gserviceaccount.com",
                "shielded_instance_config": [
                  {
                    "enable_integrity_monitoring": true,
                    "enable_secure_boot": false
                  }
                ],
                "sole_tenant_config": [],
                "spot": false,
                "tags": null,
                "taint": [],
                "workload_metadata_config": []
              }
            ],
            "node_count": 1,
            "node_locations": [
              "asia-south1-a",
              "asia-south1-b",
              "asia-south1-c"
            ],
            "operation": null,
            "placement_policy": [],
            "project": "gke-helm-431811",
            "queued_provisioning": [],
            "timeouts": null,
            "upgrade_settings": [
              {
                "blue_green_settings": [],
                "max_surge": 1,
                "max_unavailable": 0,
                "strategy": "SURGE"
              }
            ],
            "version": "1.29.6-gke.1326000"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjoxODAwMDAwMDAwMDAwLCJkZWxldGUiOjE4MDAwMDAwMDAwMDAsInVwZGF0ZSI6MTgwMDAwMDAwMDAwMH0sInNjaGVtYV92ZXJzaW9uIjoiMSJ9",
          "dependencies": [
            "google_container_cluster.primary"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "blackbox",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "prometheus-blackbox-exporter",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus-blackbox-exporter",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v0.25.0",
                "chart": "prometheus-blackbox-exporter",
                "first_deployed": 1723876161,
                "last_deployed": 1723876161,
                "name": "prometheus-blackbox-exporter",
                "namespace": "default",
                "notes": "See https://github.com/prometheus/blackbox_exporter/ for how to configure Prometheus and the Blackbox Exporter.\n\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=prometheus-blackbox-exporter,app.kubernetes.io/instance=prometheus-blackbox-exporter\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT\n",
                "revision": 1,
                "values": "{}",
                "version": "9.0.0"
              }
            ],
            "name": "prometheus-blackbox-exporter",
            "namespace": "default",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": null,
            "verify": false,
            "version": "9.0.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.google_client_config.default",
            "google_container_cluster.primary",
            "google_container_node_pool.primary_preemptible_nodes",
            "helm_release.promtest"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "grafanaexample",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "grafana",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "my-local-release",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "11.1.3",
                "chart": "grafana",
                "first_deployed": 1723876161,
                "last_deployed": 1723876161,
                "name": "my-local-release",
                "namespace": "default",
                "notes": "1. Get your 'admin' user password by running:\n\n   kubectl get secret --namespace default my-local-release-grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n\n\n2. The Grafana server can be accessed via port 80 on the following DNS name from within your cluster:\n\n   my-local-release-grafana.default.svc.cluster.local\n\n   Get the Grafana URL to visit by running these commands in the same shell:\n   NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n        You can watch the status of by running 'kubectl get svc --namespace default -w my-local-release-grafana'\n     export SERVICE_IP=$(kubectl get svc --namespace default my-local-release-grafana -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n     http://$SERVICE_IP:80\n\n3. Login with the password from step 1 and the username: admin\n#################################################################################\n######   WARNING: Persistence is disabled!!! You will lose your data when   #####\n######            the Grafana pod is terminated.                            #####\n#################################################################################\n",
                "revision": 1,
                "values": "{}",
                "version": "8.4.1"
              }
            ],
            "name": "my-local-release",
            "namespace": "default",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./custom_chart_grafana/",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": null,
            "verify": false,
            "version": "8.4.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.google_client_config.default",
            "google_container_cluster.primary",
            "google_container_node_pool.primary_preemptible_nodes",
            "helm_release.promtest"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "pingapi",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "pingapi",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "pingpongapi",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.16.0",
                "chart": "pingapi",
                "first_deployed": 1723876156,
                "last_deployed": 1723876156,
                "name": "pingpongapi",
                "namespace": "default",
                "notes": "1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=pingapi,app.kubernetes.io/instance=pingpongapi\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT\n",
                "revision": 1,
                "values": "{}",
                "version": "0.1.0"
              }
            ],
            "name": "pingpongapi",
            "namespace": "default",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./custom_chart_pingApi/",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": null,
            "verify": false,
            "version": "0.1.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.google_client_config.default",
            "google_container_cluster.primary",
            "google_container_node_pool.primary_preemptible_nodes",
            "helm_release.promtest"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "pingreq",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "pingreq",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "pingreq",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.16.0",
                "chart": "pingreq",
                "first_deployed": 1723876156,
                "last_deployed": 1723876156,
                "name": "pingreq",
                "namespace": "default",
                "notes": "1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=pingreq,app.kubernetes.io/instance=pingreq\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace default $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace default port-forward $POD_NAME 8080:$CONTAINER_PORT\n",
                "revision": 1,
                "values": "{}",
                "version": "0.1.0"
              }
            ],
            "name": "pingreq",
            "namespace": "default",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "./custom_chart_pingApi/",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": null,
            "verify": false,
            "version": "0.1.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.google_client_config.default",
            "google_container_cluster.primary",
            "google_container_node_pool.primary_preemptible_nodes",
            "helm_release.promtest"
          ]
        }
      ]
    },
    {
      "mode": "managed",
      "type": "helm_release",
      "name": "promtest",
      "provider": "provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "schema_version": 1,
          "attributes": {
            "atomic": false,
            "chart": "prometheus",
            "cleanup_on_fail": false,
            "create_namespace": false,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "prometheus",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "v2.53.1",
                "chart": "prometheus",
                "first_deployed": 1723876082,
                "last_deployed": 1723876082,
                "name": "prometheus",
                "namespace": "default",
                "notes": "1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:9093 to use your application\"\n  kubectl --namespace default port-forward $POD_NAME 9093:80\n\nkube-state-metrics is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects.\nThe exposed metrics can be found here:\nhttps://github.com/kubernetes/kube-state-metrics/blob/master/docs/README.md#exposed-metrics\n\nThe metrics are exported on the HTTP endpoint /metrics on the listening port.\nIn your case, prometheus-kube-state-metrics.default.svc.cluster.local:8080/metrics\n\nThey are served either as plaintext or protobuf depending on the Accept header.\nThey are designed to be consumed either by Prometheus itself or by a scraper that is compatible with scraping a Prometheus client endpoint.\n\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=prometheus-pushgateway,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl port-forward $POD_NAME 9091\n  echo \"Visit http://127.0.0.1:9091 to use your application\"\n\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=prometheus-node-exporter,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  echo \"Visit http://127.0.0.1:9100 to use your application\"\n  kubectl port-forward --namespace default $POD_NAME 9100\nThe Prometheus server can be accessed via port 80 on the following DNS name from within your cluster:\nprometheus-server.default.svc.cluster.local\n\n\nGet the Prometheus server URL by running these commands in the same shell:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=prometheus,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl --namespace default port-forward $POD_NAME 9090\n\n\nThe Prometheus alertmanager can be accessed via port 9093 on the following DNS name from within your cluster:\nprometheus-alertmanager.default.svc.cluster.local\n\n\nGet the Alertmanager URL by running these commands in the same shell:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app.kubernetes.io/name=alertmanager,app.kubernetes.io/instance=prometheus\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl --namespace default port-forward $POD_NAME 9093\n#################################################################################\n######   WARNING: Pod Security Policy has been disabled by default since    #####\n######            it deprecated after k8s 1.25+. use                        #####\n######            (index .Values \"prometheus-node-exporter\" \"rbac\"          #####\n###### .          \"pspEnabled\") with (index .Values                         #####\n######            \"prometheus-node-exporter\" \"rbac\" \"pspAnnotations\")       #####\n######            in case you still need it.                                #####\n#################################################################################\n\n\nThe Prometheus PushGateway can be accessed via port 9091 on the following DNS name from within your cluster:\nprometheus-prometheus-pushgateway.default.svc.cluster.local\n\n\nGet the PushGateway URL by running these commands in the same shell:\n  export POD_NAME=$(kubectl get pods --namespace default -l \"app=prometheus-pushgateway,component=pushgateway\" -o jsonpath=\"{.items[0].metadata.name}\")\n  kubectl --namespace default port-forward $POD_NAME 9091\n\nFor more information on running Prometheus, visit:\nhttps://prometheus.io/\n",
                "revision": 1,
                "values": "{\"alertRelabelConfigs\":{},\"alertmanager\":{\"enabled\":true,\"persistence\":{\"size\":\"2Gi\"},\"podSecurityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534}},\"commonMetaLabels\":{},\"configmapReload\":{\"env\":[],\"prometheus\":{\"containerPort\":8080,\"containerPortName\":\"metrics\",\"containerSecurityContext\":{},\"enabled\":true,\"extraArgs\":{},\"extraConfigmapMounts\":[],\"extraVolumeDirs\":[],\"extraVolumeMounts\":[],\"image\":{\"digest\":\"\",\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus-operator/prometheus-config-reloader\",\"tag\":\"v0.75.2\"},\"livenessProbe\":{\"httpGet\":{\"path\":\"/healthz\",\"port\":\"metrics\",\"scheme\":\"HTTP\"},\"initialDelaySeconds\":2,\"periodSeconds\":10},\"name\":\"configmap-reload\",\"readinessProbe\":{\"httpGet\":{\"path\":\"/healthz\",\"port\":\"metrics\",\"scheme\":\"HTTP\"},\"periodSeconds\":10},\"resources\":{},\"startupProbe\":{\"enabled\":false,\"httpGet\":{\"path\":\"/healthz\",\"port\":\"metrics\",\"scheme\":\"HTTP\"},\"periodSeconds\":10}},\"reloadUrl\":\"\"},\"extraManifests\":[],\"extraScrapeConfigs\":\"\",\"forceNamespace\":\"\",\"imagePullSecrets\":[],\"kube-state-metrics\":{\"enabled\":true},\"networkPolicy\":{\"enabled\":false},\"podSecurityPolicy\":{\"enabled\":false},\"prometheus-node-exporter\":{\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false},\"enabled\":true,\"rbac\":{\"pspEnabled\":false}},\"prometheus-pushgateway\":{\"enabled\":true,\"serviceAnnotations\":{\"prometheus.io/probe\":\"pushgateway\"}},\"rbac\":{\"create\":true},\"ruleFiles\":{},\"scrapeConfigFiles\":[],\"server\":{\"affinity\":{},\"alertmanagers\":[],\"baseURL\":\"\",\"clusterRoleNameOverride\":\"\",\"command\":[],\"configMapAnnotations\":{},\"configMapOverrideName\":\"\",\"configPath\":\"/etc/config/prometheus.yml\",\"containerSecurityContext\":{},\"defaultFlagsOverride\":[],\"deploymentAnnotations\":{},\"dnsConfig\":{},\"dnsPolicy\":\"ClusterFirst\",\"emptyDir\":{\"sizeLimit\":\"\"},\"enableServiceLinks\":true,\"env\":[],\"exemplars\":{},\"extraArgs\":{},\"extraConfigmapLabels\":{},\"extraConfigmapMounts\":[],\"extraFlags\":[\"web.enable-lifecycle\"],\"extraHostPathMounts\":[],\"extraInitContainers\":[],\"extraSecretMounts\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":\"\",\"global\":{\"evaluation_interval\":\"1m\",\"scrape_interval\":\"1m\",\"scrape_timeout\":\"10s\"},\"hostAliases\":[],\"hostNetwork\":false,\"image\":{\"digest\":\"\",\"pullPolicy\":\"IfNotPresent\",\"repository\":\"quay.io/prometheus/prometheus\",\"tag\":\"\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"extraLabels\":{},\"extraPaths\":[],\"hosts\":[],\"path\":\"/\",\"pathType\":\"Prefix\",\"tls\":[]},\"livenessProbeFailureThreshold\":3,\"livenessProbeInitialDelay\":30,\"livenessProbePeriodSeconds\":15,\"livenessProbeSuccessThreshold\":1,\"livenessProbeTimeout\":10,\"name\":\"server\",\"nodeSelector\":{},\"persistentVolume\":{\"accessModes\":[\"ReadWriteOnce\"],\"annotations\":{},\"enabled\":true,\"existingClaim\":\"\",\"labels\":{},\"mountPath\":\"/data\",\"size\":\"8Gi\",\"statefulSetNameOverride\":\"\",\"subPath\":\"\"},\"podAnnotations\":{},\"podAntiAffinity\":\"\",\"podAntiAffinityTopologyKey\":\"kubernetes.io/hostname\",\"podDisruptionBudget\":{\"enabled\":false,\"maxUnavailable\":1},\"podLabels\":{},\"podSecurityPolicy\":{\"annotations\":{}},\"portName\":\"\",\"prefixURL\":\"\",\"priorityClassName\":\"\",\"probeHeaders\":[],\"probeScheme\":\"HTTP\",\"readinessProbeFailureThreshold\":3,\"readinessProbeInitialDelay\":30,\"readinessProbePeriodSeconds\":5,\"readinessProbeSuccessThreshold\":1,\"readinessProbeTimeout\":4,\"releaseNamespace\":false,\"remoteRead\":[],\"remoteWrite\":[],\"replicaCount\":1,\"resources\":{},\"retention\":\"15d\",\"retentionSize\":\"\",\"revisionHistoryLimit\":10,\"securityContext\":{\"fsGroup\":65534,\"runAsGroup\":65534,\"runAsNonRoot\":true,\"runAsUser\":65534},\"service\":{\"additionalPorts\":[],\"annotations\":{},\"clusterIP\":\"\",\"enabled\":true,\"externalIPs\":[],\"gRPC\":{\"enabled\":false,\"servicePort\":10901},\"labels\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"servicePort\":80,\"sessionAffinity\":\"None\",\"statefulsetReplica\":{\"enabled\":false,\"replica\":0},\"type\":\"ClusterIP\"},\"sidecarContainers\":{},\"sidecarTemplateValues\":{},\"startupProbe\":{\"enabled\":false,\"failureThreshold\":30,\"periodSeconds\":5,\"timeoutSeconds\":10},\"statefulSet\":{\"annotations\":{},\"enabled\":false,\"headless\":{\"annotations\":{},\"gRPC\":{\"enabled\":false,\"servicePort\":10901},\"labels\":{},\"servicePort\":80},\"labels\":{},\"podManagementPolicy\":\"OrderedReady\",\"pvcDeleteOnStsDelete\":false,\"pvcDeleteOnStsScale\":false},\"storagePath\":\"\",\"strategy\":{\"type\":\"Recreate\"},\"tcpSocketProbeEnabled\":false,\"terminationGracePeriodSeconds\":300,\"tolerations\":[],\"topologySpreadConstraints\":[],\"tsdb\":{},\"verticalAutoscaler\":{\"enabled\":false}},\"serverFiles\":{\"alerting_rules.yml\":{},\"alerts\":{},\"prometheus.yml\":{\"rule_files\":[\"/etc/config/recording_rules.yml\",\"/etc/config/alerting_rules.yml\",\"/etc/config/rules\",\"/etc/config/alerts\"],\"scrape_configs\":[{\"job_name\":\"prometheus\",\"static_configs\":[{\"targets\":[\"localhost:9090\"]}]},{\"job_name\":\"blackbox-prometheus\",\"static_configs\":[{\"targets\":[\"prometheus-blackbox-exporter:9115\"]}]},{\"job_name\":\"blackbox\",\"metrics_path\":\"/probe\",\"params\":{\"module\":[\"http_2xx\"]},\"relabel_configs\":[{\"source_labels\":[\"__address__\"],\"target_label\":\"__param_target\"},{\"source_labels\":[\"__param_target\"],\"target_label\":\"instance\"},{\"replacement\":\"prometheus-blackbox-exporter:9115\",\"target_label\":\"__address__\"}],\"static_configs\":[{\"targets\":[\"http://pingpongapi-pingapi.default.svc.cluster.local:80/ping\"]}]},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-apiservers\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":\"default;kubernetes;https\",\"source_labels\":[\"__meta_kubernetes_namespace\",\"__meta_kubernetes_service_name\",\"__meta_kubernetes_endpoint_port_name\"]}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\"insecure_skip_verify\":true}},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-nodes\",\"kubernetes_sd_configs\":[{\"role\":\"node\"}],\"relabel_configs\":[{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_node_label_(.+)\"},{\"replacement\":\"kubernetes.default.svc:443\",\"target_label\":\"__address__\"},{\"regex\":\"(.+)\",\"replacement\":\"/api/v1/nodes/$1/proxy/metrics\",\"source_labels\":[\"__meta_kubernetes_node_name\"],\"target_label\":\"__metrics_path__\"}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\"insecure_skip_verify\":true}},{\"bearer_token_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/token\",\"job_name\":\"kubernetes-nodes-cadvisor\",\"kubernetes_sd_configs\":[{\"role\":\"node\"}],\"relabel_configs\":[{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_node_label_(.+)\"},{\"replacement\":\"kubernetes.default.svc:443\",\"target_label\":\"__address__\"},{\"regex\":\"(.+)\",\"replacement\":\"/api/v1/nodes/$1/proxy/metrics/cadvisor\",\"source_labels\":[\"__meta_kubernetes_node_name\"],\"target_label\":\"__metrics_path__\"}],\"scheme\":\"https\",\"tls_config\":{\"ca_file\":\"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\",\"insecure_skip_verify\":true}},{\"honor_labels\":true,\"job_name\":\"kubernetes-service-endpoints\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape\"]},{\"action\":\"drop\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(.+?)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_service_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-service-endpoints-slow\",\"kubernetes_sd_configs\":[{\"role\":\"endpoints\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(.+?)(?::\\\\d+)?;(\\\\d+)\",\"replacement\":\"$1:$2\",\"source_labels\":[\"__address__\",\"__meta_kubernetes_service_annotation_prometheus_io_port\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}],\"scrape_interval\":\"5m\",\"scrape_timeout\":\"30s\"},{\"honor_labels\":true,\"job_name\":\"prometheus-pushgateway\",\"kubernetes_sd_configs\":[{\"role\":\"service\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":\"pushgateway\",\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_probe\"]}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-services\",\"kubernetes_sd_configs\":[{\"role\":\"service\"}],\"metrics_path\":\"/probe\",\"params\":{\"module\":[\"http_2xx\"]},\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_service_annotation_prometheus_io_probe\"]},{\"source_labels\":[\"__address__\"],\"target_label\":\"__param_target\"},{\"replacement\":\"blackbox\",\"target_label\":\"__address__\"},{\"source_labels\":[\"__param_target\"],\"target_label\":\"instance\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_service_label_(.+)\"},{\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"source_labels\":[\"__meta_kubernetes_service_name\"],\"target_label\":\"service\"}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-pods\",\"kubernetes_sd_configs\":[{\"role\":\"pod\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape\"]},{\"action\":\"drop\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\",\"replacement\":\"[$2]:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);((([0-9]+?)(\\\\.|$)){4})\",\"replacement\":\"$2:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"drop\",\"regex\":\"Pending|Succeeded|Failed|Completed\",\"source_labels\":[\"__meta_kubernetes_pod_phase\"]},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}]},{\"honor_labels\":true,\"job_name\":\"kubernetes-pods-slow\",\"kubernetes_sd_configs\":[{\"role\":\"pod\"}],\"relabel_configs\":[{\"action\":\"keep\",\"regex\":true,\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow\"]},{\"action\":\"replace\",\"regex\":\"(https?)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_scheme\"],\"target_label\":\"__scheme__\"},{\"action\":\"replace\",\"regex\":\"(.+)\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_path\"],\"target_label\":\"__metrics_path__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\",\"replacement\":\"[$2]:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"replace\",\"regex\":\"(\\\\d+);((([0-9]+?)(\\\\.|$)){4})\",\"replacement\":\"$2:$1\",\"source_labels\":[\"__meta_kubernetes_pod_annotation_prometheus_io_port\",\"__meta_kubernetes_pod_ip\"],\"target_label\":\"__address__\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\",\"replacement\":\"__param_$1\"},{\"action\":\"labelmap\",\"regex\":\"__meta_kubernetes_pod_label_(.+)\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"drop\",\"regex\":\"Pending|Succeeded|Failed|Completed\",\"source_labels\":[\"__meta_kubernetes_pod_phase\"]},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node\"}],\"scrape_interval\":\"5m\",\"scrape_timeout\":\"30s\"}]},\"recording_rules.yml\":{},\"rules\":{}},\"serviceAccounts\":{\"server\":{\"annotations\":{},\"create\":true,\"name\":\"\"}}}",
                "version": "25.25.0"
              }
            ],
            "name": "prometheus",
            "namespace": "default",
            "pass_credentials": false,
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_list": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# yaml-language-server: $schema=values.schema.json\n# Default values for prometheus.\n# This is a YAML-formatted file.\n# Declare variables to be passed into your templates.\n\nrbac:\n  create: true\n\npodSecurityPolicy:\n  enabled: false\n\nimagePullSecrets: []\n# - name: \"image-pull-secret\"\n\n## Define serviceAccount names for components. Defaults to component's fully qualified name.\n##\nserviceAccounts:\n  server:\n    create: true\n    name: \"\"\n    annotations: {}\n\n    ## Opt out of automounting Kubernetes API credentials.\n    ## It will be overriden by server.automountServiceAccountToken value, if set.\n    # automountServiceAccountToken: false\n\n## Additional labels to attach to all resources\ncommonMetaLabels: {}\n\n## Monitors ConfigMap changes and POSTs to a URL\n## Ref: https://github.com/prometheus-operator/prometheus-operator/tree/main/cmd/prometheus-config-reloader\n##\nconfigmapReload:\n  ## URL for configmap-reload to use for reloads\n  ##\n  reloadUrl: \"\"\n\n  ## env sets environment variables to pass to the container. Can be set as name/value pairs,\n  ## read from secrets or configmaps.\n  env: []\n    # - name: SOMEVAR\n    #   value: somevalue\n    # - name: PASSWORD\n    #   valueFrom:\n    #     secretKeyRef:\n    #       name: mysecret\n    #       key: password\n    #       optional: false\n\n  prometheus:\n    ## If false, the configmap-reload container will not be deployed\n    ##\n    enabled: true\n\n    ## configmap-reload container name\n    ##\n    name: configmap-reload\n\n    ## configmap-reload container image\n    ##\n    image:\n      repository: quay.io/prometheus-operator/prometheus-config-reloader\n      tag: v0.75.2\n      # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).\n      digest: \"\"\n      pullPolicy: IfNotPresent\n\n    ## config-reloader's container port and port name for probes and metrics\n    containerPort: 8080\n    containerPortName: metrics\n\n    ## Additional configmap-reload container arguments\n    ## Set to null for argumentless flags\n    ##\n    extraArgs: {}\n\n    ## Additional configmap-reload volume directories\n    ##\n    extraVolumeDirs: []\n\n    ## Additional configmap-reload volume mounts\n    ##\n    extraVolumeMounts: []\n\n    ## Additional configmap-reload mounts\n    ##\n    extraConfigmapMounts: []\n      # - name: prometheus-alerts\n      #   mountPath: /etc/alerts.d\n      #   subPath: \"\"\n      #   configMap: prometheus-alerts\n      #   readOnly: true\n\n    ## Security context to be added to configmap-reload container\n    containerSecurityContext: {}\n\n    ## Settings for Prometheus reloader's readiness, liveness and startup probes\n    ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n    ##\n\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: metrics\n        scheme: HTTP\n      periodSeconds: 10\n      initialDelaySeconds: 2\n\n    readinessProbe:\n      httpGet:\n        path: /healthz\n        port: metrics\n        scheme: HTTP\n      periodSeconds: 10\n\n    startupProbe:\n      enabled: false\n      httpGet:\n        path: /healthz\n        port: metrics\n        scheme: HTTP\n      periodSeconds: 10\n\n    ## configmap-reload resource requests and limits\n    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n    ##\n    resources: {}\n\nserver:\n  ## Prometheus server container name\n  ##\n  name: server\n\n  ## Opt out of automounting Kubernetes API credentials.\n  ## If set it will override serviceAccounts.server.automountServiceAccountToken value for ServiceAccount.\n  # automountServiceAccountToken: false\n\n  ## Use a ClusterRole (and ClusterRoleBinding)\n  ## - If set to false - we define a RoleBinding in the defined namespaces ONLY\n  ##\n  ## NB: because we need a Role with nonResourceURL's (\"/metrics\") - you must get someone with Cluster-admin privileges to define this role for you, before running with this setting enabled.\n  ##     This makes prometheus work - for users who do not have ClusterAdmin privs, but wants prometheus to operate on their own namespaces, instead of clusterwide.\n  ##\n  ## You MUST also set namespaces to the ones you have access to and want monitored by Prometheus.\n  ##\n  # useExistingClusterRoleName: nameofclusterrole\n\n  ## If set it will override prometheus.server.fullname value for ClusterRole and ClusterRoleBinding\n  ##\n  clusterRoleNameOverride: \"\"\n\n  # Enable only the release namespace for monitoring. By default all namespaces are monitored.\n  # If releaseNamespace and namespaces are both set a merged list will be monitored.\n  releaseNamespace: false\n\n  ## namespaces to monitor (instead of monitoring all - clusterwide). Needed if you want to run without Cluster-admin privileges.\n  # namespaces:\n  #   - yournamespace\n\n  # sidecarContainers - add more containers to prometheus server\n  # Key/Value where Key is the sidecar `- name: \u003cKey\u003e`\n  # Example:\n  #   sidecarContainers:\n  #      webserver:\n  #        image: nginx\n  # OR for adding OAuth authentication to Prometheus\n  #   sidecarContainers:\n  #     oauth-proxy:\n  #       image: quay.io/oauth2-proxy/oauth2-proxy:v7.1.2\n  #       args:\n  #       - --upstream=http://127.0.0.1:9090\n  #       - --http-address=0.0.0.0:8081\n  #       - ...\n  #       ports:\n  #       - containerPort: 8081\n  #         name: oauth-proxy\n  #         protocol: TCP\n  #       resources: {}\n  sidecarContainers: {}\n\n  # sidecarTemplateValues - context to be used in template for sidecarContainers\n  # Example:\n  #   sidecarTemplateValues: *your-custom-globals\n  #   sidecarContainers:\n  #     webserver: |-\n  #       {{ include \"webserver-container-template\" . }}\n  # Template for `webserver-container-template` might looks like this:\n  #   image: \"{{ .Values.server.sidecarTemplateValues.repository }}:{{ .Values.server.sidecarTemplateValues.tag }}\"\n  #   ...\n  #\n  sidecarTemplateValues: {}\n\n  ## Prometheus server container image\n  ##\n  image:\n    repository: quay.io/prometheus/prometheus\n    # if not set appVersion field from Chart.yaml is used\n    tag: \"\"\n    # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).\n    digest: \"\"\n    pullPolicy: IfNotPresent\n\n  ## Prometheus server command\n  ##\n  command: []\n\n  ## prometheus server priorityClassName\n  ##\n  priorityClassName: \"\"\n\n  ## EnableServiceLinks indicates whether information about services should be injected\n  ## into pod's environment variables, matching the syntax of Docker links.\n  ## WARNING: the field is unsupported and will be skipped in K8s prior to v1.13.0.\n  ##\n  enableServiceLinks: true\n\n  ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug\n  ## so that the various internal URLs are still able to access as they are in the default case.\n  ## (Optional)\n  prefixURL: \"\"\n\n  ## External URL which can access prometheus\n  ## Maybe same with Ingress host name\n  baseURL: \"\"\n\n  ## Additional server container environment variables\n  ##\n  ## You specify this manually like you would a raw deployment manifest.\n  ## This means you can bind in environment variables from secrets.\n  ##\n  ## e.g. static environment variable:\n  ##  - name: DEMO_GREETING\n  ##    value: \"Hello from the environment\"\n  ##\n  ## e.g. secret environment variable:\n  ## - name: USERNAME\n  ##   valueFrom:\n  ##     secretKeyRef:\n  ##       name: mysecret\n  ##       key: username\n  env: []\n\n  # List of flags to override default parameters, e.g:\n  # - --enable-feature=agent\n  # - --storage.agent.retention.max-time=30m\n  # - --config.file=/etc/config/prometheus.yml\n  defaultFlagsOverride: []\n\n  extraFlags:\n    - web.enable-lifecycle\n    ## web.enable-admin-api flag controls access to the administrative HTTP API which includes functionality such as\n    ## deleting time series. This is disabled by default.\n    # - web.enable-admin-api\n    ##\n    ## storage.tsdb.no-lockfile flag controls BD locking\n    # - storage.tsdb.no-lockfile\n    ##\n    ## storage.tsdb.wal-compression flag enables compression of the write-ahead log (WAL)\n    # - storage.tsdb.wal-compression\n\n  ## Path to a configuration file on prometheus server container FS\n  configPath: /etc/config/prometheus.yml\n\n  ### The data directory used by prometheus to set --storage.tsdb.path\n  ### When empty server.persistentVolume.mountPath is used instead\n  storagePath: \"\"\n\n  global:\n    ## How frequently to scrape targets by default\n    ##\n    scrape_interval: 1m\n    ## How long until a scrape request times out\n    ##\n    scrape_timeout: 10s\n    ## How frequently to evaluate rules\n    ##\n    evaluation_interval: 1m\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_write\n  ##\n  remoteWrite: []\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#remote_read\n  ##\n  remoteRead: []\n\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#tsdb\n  ##\n  tsdb: {}\n    # out_of_order_time_window: 0s\n\n  ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/#exemplars\n  ## Must be enabled via --enable-feature=exemplar-storage\n  ##\n  exemplars: {}\n    # max_exemplars: 100000\n\n  ## Custom HTTP headers for Liveness/Readiness/Startup Probe\n  ##\n  ## Useful for providing HTTP Basic Auth to healthchecks\n  probeHeaders: []\n    # - name: \"Authorization\"\n    #   value: \"Bearer ABCDEabcde12345\"\n\n  ## Additional Prometheus server container arguments\n  ## Set to null for argumentless flags\n  ##\n  extraArgs: {}\n    # web.enable-remote-write-receiver: null\n\n  ## Additional InitContainers to initialize the pod\n  ##\n  extraInitContainers: []\n\n  ## Additional Prometheus server Volume mounts\n  ##\n  extraVolumeMounts: []\n\n  ## Additional Prometheus server Volumes\n  ##\n  extraVolumes: []\n\n  ## Additional Prometheus server hostPath mounts\n  ##\n  extraHostPathMounts: []\n    # - name: certs-dir\n    #   mountPath: /etc/kubernetes/certs\n    #   subPath: \"\"\n    #   hostPath: /etc/kubernetes/certs\n    #   readOnly: true\n\n  extraConfigmapMounts: []\n    # - name: certs-configmap\n    #   mountPath: /prometheus\n    #   subPath: \"\"\n    #   configMap: certs-configmap\n    #   readOnly: true\n\n  ## Additional Prometheus server Secret mounts\n  # Defines additional mounts with secrets. Secrets must be manually created in the namespace.\n  extraSecretMounts: []\n    # - name: secret-files\n    #   mountPath: /etc/secrets\n    #   subPath: \"\"\n    #   secretName: prom-secret-files\n    #   readOnly: true\n\n  ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}\n  ## Defining configMapOverrideName will cause templates/server-configmap.yaml\n  ## to NOT generate a ConfigMap resource\n  ##\n  configMapOverrideName: \"\"\n\n  ## Extra labels for Prometheus server ConfigMap (ConfigMap that holds serverFiles)\n  extraConfigmapLabels: {}\n\n  ## Override the prometheus.server.fullname for all objects related to the Prometheus server\n  fullnameOverride: \"\"\n\n  ingress:\n    ## If true, Prometheus server Ingress will be created\n    ##\n    enabled: false\n\n    # For Kubernetes \u003e= 1.18 you should specify the ingress-controller via the field ingressClassName\n    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress\n    # ingressClassName: nginx\n\n    ## Prometheus server Ingress annotations\n    ##\n    annotations: {}\n    #   kubernetes.io/ingress.class: nginx\n    #   kubernetes.io/tls-acme: 'true'\n\n    ## Prometheus server Ingress additional labels\n    ##\n    extraLabels: {}\n\n    ## Redirect ingress to an additional defined port on the service\n    # servicePort: 8081\n\n    ## Prometheus server Ingress hostnames with optional path\n    ## Must be provided if Ingress is enabled\n    ##\n    hosts: []\n    #   - prometheus.domain.com\n    #   - domain.com/prometheus\n\n    path: /\n\n    # pathType is only for k8s \u003e= 1.18\n    pathType: Prefix\n\n    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.\n    extraPaths: []\n    # - path: /*\n    #   backend:\n    #     serviceName: ssl-redirect\n    #     servicePort: use-annotation\n\n    ## Prometheus server Ingress TLS configuration\n    ## Secrets must be manually created in the namespace\n    ##\n    tls: []\n    #   - secretName: prometheus-server-tls\n    #     hosts:\n    #       - prometheus.domain.com\n\n  ## Server Deployment Strategy type\n  strategy:\n    type: Recreate\n\n  ## hostAliases allows adding entries to /etc/hosts inside the containers\n  hostAliases: []\n  #   - ip: \"127.0.0.1\"\n  #     hostnames:\n  #       - \"example.com\"\n\n  ## Node tolerations for server scheduling to nodes with taints\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n  ##\n  tolerations: []\n    # - key: \"key\"\n    #   operator: \"Equal|Exists\"\n    #   value: \"value\"\n    #   effect: \"NoSchedule|PreferNoSchedule|NoExecute(1.6 only)\"\n\n  ## Node labels for Prometheus server pod assignment\n  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/\n  ##\n  nodeSelector: {}\n\n  ## Pod affinity\n  ##\n  affinity: {}\n\n  ## Pod anti-affinity can prevent the scheduler from placing Prometheus server replicas on the same node.\n  ## The value \"soft\" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.\n  ## The value \"hard\" means that the scheduler is *required* to not schedule two replica pods onto the same node.\n  ## The default value \"\" will disable pod anti-affinity so that no anti-affinity rules will be configured (unless set in `server.affinity`).\n  ##\n  podAntiAffinity: \"\"\n\n  ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.\n  ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone\n  ##\n  podAntiAffinityTopologyKey: kubernetes.io/hostname\n\n  ## Pod topology spread constraints\n  ## ref. https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/\n  topologySpreadConstraints: []\n\n  ## PodDisruptionBudget settings\n  ## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/\n  ##\n  podDisruptionBudget:\n    enabled: false\n    maxUnavailable: 1\n    # minAvailable: 1\n    ## unhealthyPodEvictionPolicy is available since 1.27.0 (beta)\n    ## https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy\n    # unhealthyPodEvictionPolicy: IfHealthyBudget\n\n  ## Use an alternate scheduler, e.g. \"stork\".\n  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/\n  ##\n  # schedulerName:\n\n  persistentVolume:\n    ## If true, Prometheus server will create/use a Persistent Volume Claim\n    ## If false, use emptyDir\n    ##\n    enabled: true\n\n    ## If set it will override the name of the created persistent volume claim\n    ## generated by the stateful set.\n    ##\n    statefulSetNameOverride: \"\"\n\n    ## Prometheus server data Persistent Volume access modes\n    ## Must match those of existing PV or dynamic provisioner\n    ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/\n    ##\n    accessModes:\n      - ReadWriteOnce\n\n    ## Prometheus server data Persistent Volume labels\n    ##\n    labels: {}\n\n    ## Prometheus server data Persistent Volume annotations\n    ##\n    annotations: {}\n\n    ## Prometheus server data Persistent Volume existing claim name\n    ## Requires server.persistentVolume.enabled: true\n    ## If defined, PVC must be created manually before volume will be bound\n    existingClaim: \"\"\n\n    ## Prometheus server data Persistent Volume mount root path\n    ##\n    mountPath: /data\n\n    ## Prometheus server data Persistent Volume size\n    ##\n    size: 8Gi\n\n    ## Prometheus server data Persistent Volume Storage Class\n    ## If defined, storageClassName: \u003cstorageClass\u003e\n    ## If set to \"-\", storageClassName: \"\", which disables dynamic provisioning\n    ## If undefined (the default) or set to null, no storageClassName spec is\n    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on\n    ##   GKE, AWS \u0026 OpenStack)\n    ##\n    # storageClass: \"-\"\n\n    ## Prometheus server data Persistent Volume Binding Mode\n    ## If defined, volumeBindingMode: \u003cvolumeBindingMode\u003e\n    ## If undefined (the default) or set to null, no volumeBindingMode spec is\n    ##   set, choosing the default mode.\n    ##\n    # volumeBindingMode: \"\"\n\n    ## Subdirectory of Prometheus server data Persistent Volume to mount\n    ## Useful if the volume's root directory is not empty\n    ##\n    subPath: \"\"\n\n    ## Persistent Volume Claim Selector\n    ## Useful if Persistent Volumes have been provisioned in advance\n    ## Ref: https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector\n    ##\n    # selector:\n    #  matchLabels:\n    #    release: \"stable\"\n    #  matchExpressions:\n    #    - { key: environment, operator: In, values: [ dev ] }\n\n    ## Persistent Volume Name\n    ## Useful if Persistent Volumes have been provisioned in advance and you want to use a specific one\n    ##\n    # volumeName: \"\"\n\n  emptyDir:\n    ## Prometheus server emptyDir volume size limit\n    ##\n    sizeLimit: \"\"\n\n  ## Annotations to be added to Prometheus server pods\n  ##\n  podAnnotations: {}\n    # iam.amazonaws.com/role: prometheus\n\n  ## Labels to be added to Prometheus server pods\n  ##\n  podLabels: {}\n\n  ## Prometheus AlertManager configuration\n  ##\n  alertmanagers: []\n\n  ## Specify if a Pod Security Policy for node-exporter must be created\n  ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/\n  ##\n  podSecurityPolicy:\n    annotations: {}\n      ## Specify pod annotations\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp\n      ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl\n      ##\n      # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'\n      # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'\n      # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'\n\n  ## Use a StatefulSet if replicaCount needs to be greater than 1 (see below)\n  ##\n  replicaCount: 1\n\n  ## Number of old history to retain to allow rollback\n  ## Default Kubernetes value is set to 10\n  ##\n  revisionHistoryLimit: 10\n\n  ## Annotations to be added to ConfigMap\n  ##\n  configMapAnnotations: {}\n\n  ## Annotations to be added to deployment\n  ##\n  deploymentAnnotations: {}\n\n  statefulSet:\n    ## If true, use a statefulset instead of a deployment for pod management.\n    ## This allows to scale replicas to more than 1 pod\n    ##\n    enabled: false\n\n    annotations: {}\n    labels: {}\n    podManagementPolicy: OrderedReady\n\n    ## Alertmanager headless service to use for the statefulset\n    ##\n    headless:\n      annotations: {}\n      labels: {}\n      servicePort: 80\n      ## Enable gRPC port on service to allow auto discovery with thanos-querier\n      gRPC:\n        enabled: false\n        servicePort: 10901\n        # nodePort: 10901\n\n    ## Statefulset's persistent volume claim retention policy\n    ## pvcDeleteOnStsDelete and pvcDeleteOnStsScale determine whether\n    ## statefulset's PVCs are deleted (true) or retained (false) on scaling down\n    ## and deleting statefulset, respectively. Requires 1.27.0+.\n    ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention\n    ##\n    pvcDeleteOnStsDelete: false\n    pvcDeleteOnStsScale: false\n\n  ## Prometheus server readiness and liveness probe initial delay and timeout\n  ## Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/\n  ##\n  tcpSocketProbeEnabled: false\n  probeScheme: HTTP\n  readinessProbeInitialDelay: 30\n  readinessProbePeriodSeconds: 5\n  readinessProbeTimeout: 4\n  readinessProbeFailureThreshold: 3\n  readinessProbeSuccessThreshold: 1\n  livenessProbeInitialDelay: 30\n  livenessProbePeriodSeconds: 15\n  livenessProbeTimeout: 10\n  livenessProbeFailureThreshold: 3\n  livenessProbeSuccessThreshold: 1\n  startupProbe:\n    enabled: false\n    periodSeconds: 5\n    failureThreshold: 30\n    timeoutSeconds: 10\n\n  ## Prometheus server resource requests and limits\n  ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/\n  ##\n  resources: {}\n    # limits:\n    #   cpu: 500m\n    #   memory: 512Mi\n    # requests:\n    #   cpu: 500m\n    #   memory: 512Mi\n\n  # Required for use in managed kubernetes clusters (such as AWS EKS) with custom CNI (such as calico),\n  # because control-plane managed by AWS cannot communicate with pods' IP CIDR and admission webhooks are not working\n  ##\n  hostNetwork: false\n\n  # When hostNetwork is enabled, this will set to ClusterFirstWithHostNet automatically\n  dnsPolicy: ClusterFirst\n\n  # Use hostPort\n  # hostPort: 9090\n\n  # Use portName\n  portName: \"\"\n\n  ## Vertical Pod Autoscaler config\n  ## Ref: https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler\n  verticalAutoscaler:\n    ## If true a VPA object will be created for the controller (either StatefulSet or Deployemnt, based on above configs)\n    enabled: false\n    # updateMode: \"Auto\"\n    # containerPolicies:\n    # - containerName: 'prometheus-server'\n\n  # Custom DNS configuration to be added to prometheus server pods\n  dnsConfig: {}\n    # nameservers:\n    #   - 1.2.3.4\n    # searches:\n    #   - ns1.svc.cluster-domain.example\n    #   - my.dns.search.suffix\n    # options:\n    #   - name: ndots\n    #     value: \"2\"\n  #   - name: edns0\n\n  ## Security context to be added to server pods\n  ##\n  securityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n  ## Security context to be added to server container\n  ##\n  containerSecurityContext: {}\n\n  service:\n    ## If false, no Service will be created for the Prometheus server\n    ##\n    enabled: true\n\n    annotations: {}\n    labels: {}\n    clusterIP: \"\"\n\n    ## List of IP addresses at which the Prometheus server service is available\n    ## Ref: https://kubernetes.io/docs/concepts/services-networking/service/#external-ips\n    ##\n    externalIPs: []\n\n    loadBalancerIP: \"\"\n    loadBalancerSourceRanges: []\n    servicePort: 80\n    sessionAffinity: None\n    type: ClusterIP\n\n    ## Enable gRPC port on service to allow auto discovery with thanos-querier\n    gRPC:\n      enabled: false\n      servicePort: 10901\n      # nodePort: 10901\n\n    ## If using a statefulSet (statefulSet.enabled=true), configure the\n    ## service to connect to a specific replica to have a consistent view\n    ## of the data.\n    statefulsetReplica:\n      enabled: false\n      replica: 0\n\n    ## Additional port to define in the Service\n    additionalPorts: []\n    # additionalPorts:\n    # - name: authenticated\n    #   port: 8081\n    #   targetPort: 8081\n\n  ## Prometheus server pod termination grace period\n  ##\n  terminationGracePeriodSeconds: 300\n\n  ## Prometheus data retention period (default if not specified is 15 days)\n  ##\n  retention: \"15d\"\n\n  ## Prometheus' data retention size. Supported units: B, KB, MB, GB, TB, PB, EB.\n  ##\n  retentionSize: \"\"\n\n## Prometheus server ConfigMap entries for rule files (allow prometheus labels interpolation)\nruleFiles: {}\n\n## Prometheus server ConfigMap entries for scrape_config_files\n## (allows scrape configs defined in additional files)\n##\nscrapeConfigFiles: []\n\n## Prometheus server ConfigMap entries\n##\nserverFiles:\n  ## Alerts configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/\n  alerting_rules.yml: {}\n  # groups:\n  #   - name: Instances\n  #     rules:\n  #       - alert: InstanceDown\n  #         expr: up == 0\n  #         for: 5m\n  #         labels:\n  #           severity: page\n  #         annotations:\n  #           description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'\n  #           summary: 'Instance {{ $labels.instance }} down'\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use alerting_rules.yml\n  alerts: {}\n\n  ## Records configuration\n  ## Ref: https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/\n  recording_rules.yml: {}\n  ## DEPRECATED DEFAULT VALUE, unless explicitly naming your files, please use recording_rules.yml\n  rules: {}\n\n  prometheus.yml:\n    rule_files:\n      - /etc/config/recording_rules.yml\n      - /etc/config/alerting_rules.yml\n    ## Below two files are DEPRECATED will be removed from this default values file\n      - /etc/config/rules\n      - /etc/config/alerts\n\n    scrape_configs:\n      - job_name: prometheus\n        static_configs:\n          - targets:\n            - localhost:9090\n            \n      - job_name: blackbox-prometheus\n        static_configs:\n          - targets:\n            - prometheus-blackbox-exporter:9115\n            \n      - job_name: 'blackbox'\n        metrics_path: /probe\n        params:\n          module: [http_2xx]\n        static_configs:\n          - targets:\n            - http://pingpongapi-pingapi.default.svc.cluster.local:80/ping\n        relabel_configs:\n          - source_labels: [__address__]\n            target_label: __param_target\n          - source_labels: [__param_target]\n            target_label: instance\n          - target_label: __address__\n            replacement: prometheus-blackbox-exporter:9115  # The blackbox exporter's real hostname:port.\n\n      # A scrape configuration for running Prometheus on a Kubernetes cluster.\n      # This uses separate scrape configs for cluster components (i.e. API server, node)\n      # and services to allow each to use different authentication configs.\n      #\n      # Kubernetes labels will be added as Prometheus labels on metrics via the\n      # `labelmap` relabeling action.\n\n      # Scrape config for API servers.\n      #\n      # Kubernetes exposes API servers as endpoints to the default/kubernetes\n      # service so this uses `endpoints` role and uses relabelling to only keep\n      # the endpoints associated with the default/kubernetes service using the\n      # default named port `https`. This works for single API server deployments as\n      # well as HA API server deployments.\n      - job_name: 'kubernetes-apiservers'\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        # Keep only the default/kubernetes service endpoints for the https port. This\n        # will add targets for each API server which Kubernetes adds an endpoint to\n        # the default/kubernetes service.\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]\n            action: keep\n            regex: default;kubernetes;https\n\n      - job_name: 'kubernetes-nodes'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics\n\n\n      - job_name: 'kubernetes-nodes-cadvisor'\n\n        # Default to scraping over https. If required, just disable this or change to\n        # `http`.\n        scheme: https\n\n        # This TLS \u0026 bearer token file config is used to connect to the actual scrape\n        # endpoints for cluster components. This is separate to discovery auth\n        # configuration because discovery \u0026 scraping are two separate concerns in\n        # Prometheus. The discovery auth config is automatic if Prometheus runs inside\n        # the cluster. Otherwise, more config options have to be provided within the\n        # \u003ckubernetes_sd_config\u003e.\n        tls_config:\n          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n          # If your node certificates are self-signed or use a different CA to the\n          # master CA, then disable certificate verification below. Note that\n          # certificate verification is an integral part of a secure infrastructure\n          # so this should only be disabled in a controlled environment. You can\n          # disable certificate verification by uncommenting the line below.\n          #\n          insecure_skip_verify: true\n        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token\n\n        kubernetes_sd_configs:\n          - role: node\n\n        # This configuration will work only on kubelet 1.7.3+\n        # As the scrape endpoints for cAdvisor have changed\n        # if you are using older version you need to change the replacement to\n        # replacement: /api/v1/nodes/$1:4194/proxy/metrics\n        # more info here https://github.com/coreos/prometheus-operator/issues/633\n        relabel_configs:\n          - action: labelmap\n            regex: __meta_kubernetes_node_label_(.+)\n          - target_label: __address__\n            replacement: kubernetes.default.svc:443\n          - source_labels: [__meta_kubernetes_node_name]\n            regex: (.+)\n            target_label: __metrics_path__\n            replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor\n\n        # Metric relabel configs to apply to samples before ingestion.\n        # [Metric Relabeling](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)\n        # metric_relabel_configs:\n        # - action: labeldrop\n        #   regex: (kubernetes_io_hostname|failure_domain_beta_kubernetes_io_region|beta_kubernetes_io_os|beta_kubernetes_io_arch|beta_kubernetes_io_instance_type|failure_domain_beta_kubernetes_io_zone)\n\n      # Scrape config for service endpoints.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape services that have a value of\n      # `true`, except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: (.+?)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      # Scrape config for slow service endpoints; same as above, but with a larger\n      # timeout and a larger interval\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape services that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: If the metrics are exposed on a different port to the\n      # service then set this appropriately.\n      # * `prometheus.io/param_\u003cparameter\u003e`: If the metrics endpoint uses parameters\n      # then you can set any parameter\n      - job_name: 'kubernetes-service-endpoints-slow'\n        honor_labels: true\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: endpoints\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]\n            action: replace\n            target_label: __scheme__\n            regex: (https?)\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]\n            action: replace\n            target_label: __address__\n            regex: (.+?)(?::\\d+)?;(\\d+)\n            replacement: $1:$2\n          - action: labelmap\n            regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            action: replace\n            target_label: service\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      - job_name: 'prometheus-pushgateway'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: pushgateway\n\n      # Example scrape config for probing services via the Blackbox Exporter.\n      #\n      # The relabeling allows the actual service scrape endpoint to be configured\n      # via the following annotations:\n      #\n      # * `prometheus.io/probe`: Only probe services that have a value of `true`\n      - job_name: 'kubernetes-services'\n        honor_labels: true\n\n        metrics_path: /probe\n        params:\n          module: [http_2xx]\n\n        kubernetes_sd_configs:\n          - role: service\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]\n            action: keep\n            regex: true\n          - source_labels: [__address__]\n            target_label: __param_target\n          - target_label: __address__\n            replacement: blackbox\n          - source_labels: [__param_target]\n            target_label: instance\n          - action: labelmap\n            regex: __meta_kubernetes_service_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_service_name]\n            target_label: service\n\n      # Example scrape config for pods\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`,\n      # except if `prometheus.io/scrape-slow` is set to `true` as well.\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods'\n        honor_labels: true\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: drop\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\n            replacement: '[$2]:$1'\n            target_label: __address__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);((([0-9]+?)(\\.|$)){4})\n            replacement: $2:$1\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n      # Example Scrape config for pods which should be scraped slower. An useful example\n      # would be stackriver-exporter which queries an API on every scrape of the pod\n      #\n      # The relabeling allows the actual pod scrape endpoint to be configured via the\n      # following annotations:\n      #\n      # * `prometheus.io/scrape-slow`: Only scrape pods that have a value of `true`\n      # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need\n      # to set this to `https` \u0026 most likely set the `tls_config` of the scrape config.\n      # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.\n      # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.\n      - job_name: 'kubernetes-pods-slow'\n        honor_labels: true\n\n        scrape_interval: 5m\n        scrape_timeout: 30s\n\n        kubernetes_sd_configs:\n          - role: pod\n\n        relabel_configs:\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape_slow]\n            action: keep\n            regex: true\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]\n            action: replace\n            regex: (https?)\n            target_label: __scheme__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n            action: replace\n            target_label: __metrics_path__\n            regex: (.+)\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})\n            replacement: '[$2]:$1'\n            target_label: __address__\n          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_port, __meta_kubernetes_pod_ip]\n            action: replace\n            regex: (\\d+);((([0-9]+?)(\\.|$)){4})\n            replacement: $2:$1\n            target_label: __address__\n          - action: labelmap\n            regex: __meta_kubernetes_pod_annotation_prometheus_io_param_(.+)\n            replacement: __param_$1\n          - action: labelmap\n            regex: __meta_kubernetes_pod_label_(.+)\n          - source_labels: [__meta_kubernetes_namespace]\n            action: replace\n            target_label: namespace\n          - source_labels: [__meta_kubernetes_pod_name]\n            action: replace\n            target_label: pod\n          - source_labels: [__meta_kubernetes_pod_phase]\n            regex: Pending|Succeeded|Failed|Completed\n            action: drop\n          - source_labels: [__meta_kubernetes_pod_node_name]\n            action: replace\n            target_label: node\n\n# adds additional scrape configs to prometheus.yml\n# must be a string so you have to add a | after extraScrapeConfigs:\n# example adds prometheus-blackbox-exporter scrape config\nextraScrapeConfigs: \"\"\n  # - job_name: 'prometheus-blackbox-exporter'\n  #   metrics_path: /probe\n  #   params:\n  #     module: [http_2xx]\n  #   static_configs:\n  #     - targets:\n  #       - https://example.com\n  #   relabel_configs:\n  #     - source_labels: [__address__]\n  #       target_label: __param_target\n  #     - source_labels: [__param_target]\n  #       target_label: instance\n  #     - target_label: __address__\n  #       replacement: prometheus-blackbox-exporter:9115\n\n# Adds option to add alert_relabel_configs to avoid duplicate alerts in alertmanager\n# useful in H/A prometheus with different external labels but the same alerts\nalertRelabelConfigs: {}\n  # alert_relabel_configs:\n  # - source_labels: [dc]\n  #   regex: (.+)\\d+\n  #   target_label: dc\n\nnetworkPolicy:\n  ## Enable creation of NetworkPolicy resources.\n  ##\n  enabled: false\n\n# Force namespace of namespaced resources\nforceNamespace: \"\"\n\n# Extra manifests to deploy as an array\nextraManifests: []\n  # - |\n  #   apiVersion: v1\n  #   kind: ConfigMap\n  #   metadata:\n  #   labels:\n  #     name: prometheus-extra\n  #   data:\n  #     extra-data: \"value\"\n\n# Configuration of subcharts defined in Chart.yaml\n\n## alertmanager sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/alertmanager\n##\nalertmanager:\n  ## If false, alertmanager will not be installed\n  ##\n  enabled: true\n\n  persistence:\n    size: 2Gi\n\n  podSecurityContext:\n    runAsUser: 65534\n    runAsNonRoot: true\n    runAsGroup: 65534\n    fsGroup: 65534\n\n## kube-state-metrics sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-state-metrics\n##\nkube-state-metrics:\n  ## If false, kube-state-metrics sub-chart will not be installed\n  ##\n  enabled: true\n\n## prometheus-node-exporter sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-node-exporter\n##\nprometheus-node-exporter:\n  ## If false, node-exporter will not be installed\n  ##\n  enabled: true\n\n  rbac:\n    pspEnabled: false\n\n  containerSecurityContext:\n    allowPrivilegeEscalation: false\n\n## prometheus-pushgateway sub-chart configurable values\n## Please see https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-pushgateway\n##\nprometheus-pushgateway:\n  ## If false, pushgateway will not be installed\n  ##\n  enabled: true\n\n  # Optional service annotations\n  serviceAnnotations:\n    prometheus.io/probe: pushgateway\n"
            ],
            "verify": false,
            "version": "25.25.0",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [
            [
              {
                "type": "get_attr",
                "value": "repository_password"
              }
            ]
          ],
          "private": "eyJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "data.google_client_config.default",
            "google_container_cluster.primary",
            "google_container_node_pool.primary_preemptible_nodes"
          ]
        }
      ]
    }
  ],
  "check_results": null
}
